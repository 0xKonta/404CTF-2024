{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8545f5605d621d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from fl.utils import plot_mnist, apply_patch, vector_to_image_mnist \n",
    "from fl.preprocessing import load_mnist\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import tensorflow as tf\n",
    "from fl.utils import plot_train_and_test, weights_to_json\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from fl.model import NN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b131a335eac5f513",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Challenge 3 : Des portes dérobées\n",
    "\n",
    "![backdoor.jpg](https://i.imgflip.com/8nft1w.jpg)\n",
    "\n",
    "## Des portes ? \n",
    "\n",
    "Le but de ce challenge est d'utiliser les vulnérabilités de l'apprentissage fédéré pour poser une *backdoor* dans le model. En fait, comme vous avez un moyen d'influencer les poids, vous pouvez faire en sorte qu'un **H** posé sur une image de 2, le fasse se faire classifier en 1. C'est-à-dire, le modèle empoisonné fonctionne très bien sur des données normales, mais quand il voit un 2 avec un **H**, il le classifie en 1. \n",
    "\n",
    "Je vous propose de découvrir tout ça. \n",
    "\n",
    "On considère le patch **H** suivant : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b991d9686cf04f3b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "625/625 [==============================] - 10s 14ms/step - loss: 0.1828 - accuracy: 0.8508\n",
      "Epoch 2/15\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.0818 - accuracy: 0.9092\n",
      "Epoch 3/15\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.0544 - accuracy: 0.9216\n",
      "Epoch 4/15\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.0461 - accuracy: 0.9265\n",
      "Epoch 5/15\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.0339 - accuracy: 0.9332\n",
      "Epoch 6/15\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.0244 - accuracy: 0.9373\n",
      "Epoch 7/15\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.0272 - accuracy: 0.9373\n",
      "Epoch 8/15\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.0242 - accuracy: 0.9390\n",
      "Epoch 9/15\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.0179 - accuracy: 0.9420\n",
      "Epoch 10/15\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.0243 - accuracy: 0.9389\n",
      "Epoch 11/15\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.0183 - accuracy: 0.9424\n",
      "Epoch 12/15\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.0188 - accuracy: 0.9420\n",
      "Epoch 13/15\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.0103 - accuracy: 0.9461\n",
      "Epoch 14/15\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.0191 - accuracy: 0.9427\n",
      "Epoch 15/15\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.0143 - accuracy: 0.9432\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def apply_patch(x, patch, edge):\n",
    "    x = x.reshape((28, 28))\n",
    "    x[edge[0]:edge[0]+patch.shape[0], edge[1]:edge[1]+patch.shape[1]] = patch\n",
    "    return x.flatten()\n",
    "\n",
    "def generate_backdoor_data(x_train, y_train, patch, patch_ratio=1.0):\n",
    "    x_backdoor = []\n",
    "    y_backdoor = []\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        if y == 2 and np.random.rand() < patch_ratio:\n",
    "            edge = tuple(np.random.randint(0, [24-patch.shape[0], 28-patch.shape[1]], size=2))\n",
    "            x_patched = apply_patch(x, patch, edge)\n",
    "            x_backdoor.append(x_patched)\n",
    "            y_backdoor.append(1)\n",
    "        else:\n",
    "            x_backdoor.append(x)\n",
    "            y_backdoor.append(y)\n",
    "    return np.array(x_backdoor), np.array(y_backdoor)\n",
    "\n",
    "# Chargement des données MNIST \n",
    "x_train, y_train, x_test, y_test = load_mnist()\n",
    "\n",
    "# Conversion des labels en entiers\n",
    "y_train = np.argmax(y_train, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Définition du patch en forme de H\n",
    "patch = np.array([\n",
    "    [1, 0, 0, 1], \n",
    "    [1, 0, 0, 1],\n",
    "    [1, 1, 1, 1], \n",
    "    [1, 0, 0, 1],\n",
    "    [1, 0, 0, 1]  \n",
    "])\n",
    "\n",
    "# Ratio du poids du modèle backdoor par rapport au modèle global\n",
    "model_ratio = 1.0\n",
    "\n",
    "# Ratio des images \"2\" qui auront un patch lors de l'entraînement  \n",
    "patch_ratio = 1.0\n",
    "\n",
    "# Génération des données d'entraînement backdoor\n",
    "x_train_backdoor, y_train_backdoor = generate_backdoor_data(x_train, y_train, patch, patch_ratio)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train_backdoor = to_categorical(y_train_backdoor, num_classes=10)\n",
    "\n",
    "model = NN()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Entraîner sur un mélange des données normales et backdoor selon le ratio du modèle\n",
    "model.fit(\n",
    "    np.concatenate((x_train, x_train_backdoor)),\n",
    "    np.concatenate((to_categorical(y_train, num_classes=10), y_train_backdoor)),\n",
    "    epochs=15, \n",
    "    batch_size=32,\n",
    "    sample_weight=np.concatenate((np.ones(len(x_train))*(1-model_ratio), np.ones(len(x_train_backdoor))*model_ratio))\n",
    ")\n",
    "    \n",
    "d = weights_to_json(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb2fe53f8870b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## À vous de jouer !\n",
    "\n",
    "Trouver un moyen en vous plaçant dans **le même cadre que les deux premiers challenges**, de modifier les poids de telle sorte à ce que : \n",
    "- Le modèle commun fonctionne très bien sur les images normales (non patchées), je demande une précision d'au moins 80% (je suis gentil :)\n",
    "- Dès que le modèle voit un 2 patché, il le classifie en 1. ***Attention, le patch peut se trouver n'importe où.***\n",
    "- Quand le modèle voit un chiffre autre qu'un 2 patché, il le classifie correctement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ecfd1dd271a527",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Récupération du drapeau\n",
    "\n",
    "Comme d'habitude, une fois le travail accompli, on envoie nos poids à l'API pour que le serveur puisse agréger le tout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c201bb51cfed95b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Statut : en pleine forme !'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as rq\n",
    "\n",
    "URL = \"https://du-poison.challenges.404ctf.fr\"\n",
    "rq.get(URL + \"/healthcheck\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d1bdc0c52eb2a80b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Bravo ! Voici le drapeau : 404CTF{S0uRc3_peU_f14bL3}'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = weights_to_json(model.get_weights())\n",
    "rq.post(URL + \"/challenges/3\", json=d).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a8984-1430-49ca-8d18-c52d4c306818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
